# ğŸ“Š Bill Processing Flow Chart

## Current Bill Processing Pipeline

```
ğŸ“ Scraped Data (OpenStates)
    â†“
ğŸ” main.py
    â†“
ğŸ“– read_latest_timestamps() â†’ Load repository-level timestamps
    â†“
ğŸ“‚ load_json_files()
    â†“
ğŸ” For each bill_*.json file:
    â†“
âš ï¸  is_newer_than_latest(data, bills_ts, "bills")
    â†“ (if TRUE)
ğŸ“‹ process_and_save()
    â†“
ğŸ”€ route_handler() â†’ bill.handle_bill()
    â†“
ğŸ“ handle_bill():
    â”œâ”€â”€ Validate bill identifier
    â”œâ”€â”€ Create bill folder structure
    â”œâ”€â”€ Process actions:
    â”‚   â”œâ”€â”€ Extract dates from actions
    â”‚   â”œâ”€â”€ Update latest_timestamps["bills"]
    â”‚   â””â”€â”€ write_action_logs() â†’ Create individual action JSON files
    â”œâ”€â”€ Save metadata.json (complete bill data)
    â””â”€â”€ Return success/failure
    â†“
ğŸ“Š Update counts and write_latest_timestamp_file()
```

## ğŸš¨ Critical Points Where Repository-Level Timestamps Cause Issues

### **1. Filtering Stage (`load_json_files`)**

```python
# Current problematic logic:
if filename.startswith("bill"):
    if not is_newer_than_latest(data, bills_ts, "bills", DATA_NOT_PROCESSED_FOLDER):
        continue  # âŒ BILL GETS SKIPPED FOREVER
```

**Problem**: If a bill has actions older than `bills_ts`, the entire bill gets filtered out.

### **2. Timestamp Update (`handle_bill`)**

```python
# Current logic updates repository-level timestamp:
dates = [a.get("date") for a in actions if a.get("date")]
timestamp = format_timestamp(sorted(dates)[0])  # Oldest action date
latest_timestamps["bills"] = update_latest_timestamp(
    "bills", current_dt, latest_timestamps["bills"], latest_timestamps
)
```

**Problem**: Updates repository timestamp to oldest action, not newest.

### **3. Action Processing (`write_action_logs`)**

```python
# Current logic processes ALL actions every time:
write_action_logs(actions, bill_identifier, save_path / "logs")
```

**Problem**: Recreates all action log files, even if they already exist.

---

## ğŸ¯ Where to Implement Action-Level Tracking

### **Stage 1: Replace Repository-Level Filtering**

**Location**: `load_json_files()` in `io_utils.py`
**Current**: Filter entire bill based on repository timestamp
**New**: Load all bills, let bill handler decide what to process

### **Stage 2: Add Action-Level Comparison**

**Location**: `handle_bill()` in `bill.py`
**Current**: Process all actions every time
**New**: Compare incoming actions with existing metadata, process only new actions

### **Stage 3: Preserve Processing Timestamps**

**Location**: `write_action_logs()` in `file_utils.py`
**Current**: Create new action files every time
**New**: Only create files for new actions, preserve existing ones

### **Stage 4: Update Metadata with Processing Info**

**Location**: `handle_bill()` in `bill.py`
**Current**: Save raw metadata.json
**New**: Add `_processing` fields to actions before saving

---

## ğŸ”„ New Processing Flow (Hybrid Approach)

```
ğŸ“ Scraped Data (OpenStates)
    â†“
ğŸ” main.py
    â†“
ğŸ“– read_latest_timestamps() â†’ Load repository-level timestamps (for migration)
    â†“
ğŸ“‚ load_json_files_hybrid() [SMART FILTERING]:
    â”œâ”€â”€ For each bill_*.json file:
    â”œâ”€â”€ Load existing metadata.json (if exists)
    â”œâ”€â”€ Compare action counts:
    â”‚   â”œâ”€â”€ Same count â†’ Skip bill (likely no changes)
    â”‚   â”œâ”€â”€ Higher count â†’ Process bill (new actions)
    â”‚   â””â”€â”€ Lower count â†’ Process bill (handle deletions)
    â””â”€â”€ Only pass bills with changes to processing
    â†“
ğŸ“‹ process_and_save()
    â†“
ğŸ”€ route_handler() â†’ bill.handle_bill()
    â†“
ğŸ“ handle_bill_hybrid() [ACTION-LEVEL PROCESSING]:
    â”œâ”€â”€ Load existing metadata.json (if exists)
    â”œâ”€â”€ Find new actions (count difference):
    â”‚   â”œâ”€â”€ new_actions_count = incoming_count - existing_count
    â”‚   â”œâ”€â”€ new_actions = incoming_actions[-new_actions_count:]
    â”‚   â””â”€â”€ Only process new actions
    â”œâ”€â”€ write_action_logs() â†’ Create files only for new actions
    â”œâ”€â”€ Add _processing fields to new actions:
    â”‚   â”œâ”€â”€ "log_file_created": timestamp
    â”‚   â””â”€â”€ "text_extracted": null (for text extraction)
    â”œâ”€â”€ Merge metadata:
    â”‚   â”œâ”€â”€ Keep existing actions with _processing fields
    â”‚   â”œâ”€â”€ Add new actions with new _processing fields
    â”‚   â””â”€â”€ Save updated metadata.json
    â””â”€â”€ Return success/failure
    â†“
ğŸ“Š Update counts (no timestamp file updates needed)
```

## ğŸ› ï¸ Implementation Strategy

### **Phase 1: Modify `load_json_files()`**

- Remove repository-level timestamp filtering for bills
- Load all bill files regardless of timestamp

### **Phase 2: Update `handle_bill()`**

- Load existing metadata.json if it exists
- Implement action comparison logic
- Only process new actions
- Preserve existing `_processing` fields

### **Phase 3: Update `write_action_logs()`**

- Check if action log file already exists
- Only create files for new actions
- Preserve existing action log files

### **Phase 4: Add `_processing` Fields**

- Add `_processing` field to new actions
- Include `log_file_created` and `text_extracted` timestamps
- Merge with existing metadata before saving

---

## ğŸ“Š Benefits of New Approach

âœ… **No more skipped bills** - All bills get processed
âœ… **No unnecessary reprocessing** - Only new actions get processed
âœ… **Preserve processing history** - Never lose timestamp information
âœ… **Faster processing** - Skip existing actions
âœ… **Reliable incremental updates** - Handle any timestamp scenario

---

## ğŸ”— Text Extraction Integration

### **Current Text Extraction Flow:**

```
ğŸ“ Processed Bills (with metadata.json)
    â†“
ğŸ” text_extraction/main.py
    â†“
ğŸ“‚ Scan all bills for text extraction
    â†“
ğŸ“„ For each bill:
    â”œâ”€â”€ Download PDFs/XMLs from action URLs
    â”œâ”€â”€ Extract text using pdfplumber/PyPDF2
    â”œâ”€â”€ Save extracted text files
    â””â”€â”€ Update metadata with text extraction timestamps
```

### **New Text Extraction Flow (Incremental):**

```
ğŸ“ Processed Bills (with _processing fields)
    â†“
ğŸ” text_extraction/main.py
    â†“
ğŸ“‚ Scan bills for actions needing text extraction
    â†“
ğŸ“„ For each bill:
    â”œâ”€â”€ Load existing metadata.json
    â”œâ”€â”€ Find actions where text_extracted is null
    â”œâ”€â”€ Download PDFs/XMLs for new actions only
    â”œâ”€â”€ Extract text using pdfplumber/PyPDF2
    â”œâ”€â”€ Save extracted text files
    â”œâ”€â”€ Update action _processing.text_extracted timestamp
    â””â”€â”€ Save updated metadata.json
```

### **Text Extraction Integration Points:**

#### **1. Two-Level Text Extraction Tracking**

```json
{
  "_processing": {
    "logs_latest_update": "2025-10-14T01:30:00Z",
    "extraction_latest_update": "2025-10-14T02:15:00Z" // â† Bill-level timestamp
  },
  "actions": [
    {
      "description": "Became Public Law No: 119-21.",
      "date": "2025-07-04T04:00:00+00:00",
      "_processing": {
        "log_file_created": "2025-10-14T01:30:00Z",
        "text_extracted": "2025-10-14T02:15:00Z" // â† Action-level timestamp
      }
    }
  ]
}
```

**Bill-Level `_processing`:**

- `logs_latest_update` - When logs were last created for this bill
- `extraction_latest_update` - When text was last extracted for this bill
- **Purpose**: Fast filtering to skip bills that don't need processing

**Action-Level `_processing`:**

- `log_file_created` - When the log file was created for this action
- `text_extracted` - When text was extracted for this action
- **Purpose**: Granular tracking of which actions have been processed

#### **2. Text Extraction Logic (with Fast Filtering)**

```python
def process_bill_text_extraction(bill_path):
    metadata = load_metadata(bill_path / "metadata.json")

    # Quick filter: skip if extracted recently (within last 24 hours)
    extraction_updated = metadata.get("_processing", {}).get("extraction_latest_update")
    if extraction_updated and is_recent(extraction_updated, hours=24):
        return  # Skip this entire bill

    needs_update = False

    for action in metadata.get("actions", []):
        # Check if text extraction is needed
        processing = action.get("_processing", {})
        if not processing.get("text_extracted"):
            # Extract text for this action
            try:
                extract_action_text(action)
                # Update action-level timestamp
                processing["text_extracted"] = datetime.now().isoformat() + "Z"
                needs_update = True
            except Exception as e:
                log_error(f"Failed to extract text: {e}")
                # Don't add timestamp - will retry next night

    # Update bill-level timestamp if we processed any actions
    if needs_update:
        if "_processing" not in metadata:
            metadata["_processing"] = {}
        metadata["_processing"]["extraction_latest_update"] = datetime.now().isoformat() + "Z"

        # Save updated metadata
        save_metadata(metadata)
```

#### **3. Benefits for Text Extraction:**

- âœ… **Fast filtering** - Bill-level timestamp skips entire bills that don't need processing
- âœ… **Only extract text for new actions** - Skip existing ones
- âœ… **Preserve extraction history** - Never lose text extraction timestamps
- âœ… **Faster text extraction** - Skip actions already processed
- âœ… **Reliable incremental updates** - Handle any text extraction scenario
- âœ… **Error recovery** - Failed extractions automatically retry next night
- âœ… **Independent workflows** - Each workflow tracks its own progress

---

**Last Updated**: October 16, 2025
**Branch**: `incremental-processing`
