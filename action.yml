name: 'OpenStates Data Pipeline'
description: 'Complete pipeline to scrape, sanitize, and format OpenStates data'

inputs:
  state:
    description: 'State abbreviation (e.g., id, il, tx, ny)'
    required: true
  allow-session-fix:
    description: 'Allow interactive session fixes when session names are missing'
    required: false
    default: 'false'
  github-token:
    description: 'GitHub token for creating releases'
    required: true
  use-latest-release:
    description: 'Skip scraping and download from latest sanitized data release instead'
    required: false
    default: 'false'

runs:
  using: 'composite'
  steps:
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.13"

    - name: Setup Working Directory
      shell: bash
      run: |
        # Create working directories
        mkdir -p ${RUNNER_TEMP}/_working/_data
        mkdir -p ${RUNNER_TEMP}/_working/_cache

    - name: Cache Scrapes
      uses: actions/cache@v4
      with:
        path: ${{ runner.temp }}/_working/_cache
        key: cache-scrapes-key-${{ github.run_id }}
        restore-keys: |
          cache-scrapes-key-

    - name: Download Latest Sanitized Data Release
      if: inputs.use-latest-release == 'true'
      shell: bash
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      run: |
        # Ensure directories exist
        mkdir -p ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
        
        # Get the latest release for this state
        LATEST_RELEASE=$(gh release list --repo openstates/openstates-scrapers --limit 1 --search "sanitized-data-${{ inputs.state }}" --json tagName --jq '.[0].tagName')
        
        if [ "$LATEST_RELEASE" = "null" ] || [ -z "$LATEST_RELEASE" ]; then
          echo "No sanitized data release found for ${{ inputs.state }}. Falling back to scraping."
          echo "use-latest-release=false" >> $GITHUB_OUTPUT
        else
          echo "Found latest release: $LATEST_RELEASE"
          
          # Download the release assets
          gh release download $LATEST_RELEASE --repo openstates/openstates-scrapers --dir ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
          
          # Extract the zip file if it exists
          if [ -f "${RUNNER_TEMP}/_working/_data/${{ inputs.state }}/sanitized-data-${{ inputs.state }}.zip" ]; then
            cd ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
            unzip sanitized-data-${{ inputs.state }}.zip
            rm sanitized-data-${{ inputs.state }}.zip
          fi
          
          echo "Successfully downloaded sanitized data from release $LATEST_RELEASE"
        fi

    - name: Clone and Build Scrapers from Source
      if: inputs.use-latest-release != 'true'
      shell: bash
      run: |
        git clone https://github.com/openstates/openstates-scrapers.git
        cd openstates-scrapers
        docker build -t openstates/scrapers:dev .

    - name: Scrape Data
      if: inputs.use-latest-release != 'true'
      shell: bash
      run: |
        # Ensure directories exist
        mkdir -p ${RUNNER_TEMP}/_working/_data
        mkdir -p ${RUNNER_TEMP}/_working/_cache
        
        docker run \
            -v "${RUNNER_TEMP}/_working/_data":/opt/openstates/openstates/_data \
            -v "${RUNNER_TEMP}/_working/_cache":/opt/openstates/openstates/_cache \
            openstates/scrapers:dev \
            $STATE bills --scrape --fastmode

    - name: Sanitize Data (removes _id and scraped_at)
      if: inputs.use-latest-release != 'true'
      shell: bash
      run: |
        # Ensure directories exist with proper permissions
        mkdir -p ${RUNNER_TEMP}/_working/_data
        
        # Sanitize JSON files with proper error handling
        find ${RUNNER_TEMP}/_working/_data -type f -name "*.json" -exec bash -c '
          if [ -f "{}" ]; then
            jq "del(..|._id?, .scraped_at?)" "{}" > "{}.tmp" && mv "{}.tmp" "{}"
          fi
        ' \;

    - name: Upload Sanitized Data as Release
      if: inputs.use-latest-release != 'true'
      shell: bash
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      run: |
        # Ensure directories exist
        mkdir -p ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
        
        # Create a zip file with all sanitized JSON files
        cd ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
        
        # Check if JSON files exist before creating zip
        if [ -n "$(ls -A *.json 2>/dev/null)" ]; then
          zip -r sanitized-data-${{ inputs.state }}.zip *.json
          
          # Create a release with the zip file
          gh release create "sanitized-data-${{ inputs.state }}-$(date +%Y%m%d-%H%M%S)" \
            --title "Sanitized OCD Data for ${{ inputs.state }}" \
            --notes "Sanitized OCD data for ${{ inputs.state }} state. Removed _id and scraped_at fields." \
            sanitized-data-${{ inputs.state }}.zip
        else
          echo "No JSON files found to zip for state ${{ inputs.state }}"
        fi

    - name: Install dependencies
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        pip install pipenv
        pipenv install --deploy

    - name: Format scraped data
      shell: bash
      working-directory: ./openstates_scraped_data_formatter
      env:
        FORMATTER_INPUT_FOLDER: ${{ runner.temp }}/_working/_data/${{ inputs.state }}
      run: |
        pipenv run python main.py \
          --state ${{ inputs.state }} \
          --input-folder ${{ runner.temp }}/_working/_data/${{ inputs.state }} \
          --allow-session-fix ${{ inputs.allow-session-fix }}

    - name: Upload Processed Data as Artifact
      uses: actions/upload-artifact@v4
      with:
        name: processed-data-${{ inputs.state }}
        path: data_output/