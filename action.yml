name: 'OpenStates Data Pipeline'
description: 'Complete pipeline to scrape, sanitize, and format OpenStates data'

inputs:
  state:
    description: 'State abbreviation (e.g., id, il, tx, ny)'
    required: true
  allow-session-fix:
    description: 'Allow interactive session fixes when session names are missing'
    required: false
    default: 'false'
  use-scrape-cache:
    description: 'Skip scraping and use the most recent artifact from this repository instead'
    required: false
    default: 'false'

runs:
  using: 'composite'
  steps:
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.13"

    - name: Setup Working Directory
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🔧 Setting up working directories..."
        # Create working directories
        mkdir -p ${RUNNER_TEMP}/_working/_data
        mkdir -p ${RUNNER_TEMP}/_working/_cache
        echo "✅ Working directories created successfully"

    - name: Cache Scrapes
      uses: actions/cache@v4
      with:
        path: ${{ runner.temp }}/_working/_cache
        key: cache-scrapes-${{ inputs.state }}-${{ github.sha }}
        restore-keys: |
          cache-scrapes-${{ inputs.state }}-
          cache-scrapes-

    - name: Clone and Build Scrapers from Source
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🔧 Cloning OpenStates scrapers repository..."
        git clone https://github.com/openstates/openstates-scrapers.git
        echo "📦 Building Docker image for scrapers..."
        cd openstates-scrapers
        docker build -t openstates/scrapers:dev .
        echo "✅ Docker image built successfully"

    - name: Scrape Data
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🕷️ Starting data scraping for ${{ inputs.state }}..."
        # Ensure directories exist
        mkdir -p ${RUNNER_TEMP}/_working/_data
        mkdir -p ${RUNNER_TEMP}/_working/_cache
        
        echo "🐳 Running scraper in Docker container..."
        docker run \
            -v "${RUNNER_TEMP}/_working/_data":/opt/openstates/openstates/_data \
            -v "${RUNNER_TEMP}/_working/_cache":/opt/openstates/openstates/_cache \
            openstates/scrapers:dev \
            ${{ inputs.state }} bills --scrape --fastmode
        echo "✅ Data scraping completed"

    - name: Zip OCD Scraped Data
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "📦 Zipping OCD scraped data..."
        cd ${RUNNER_TEMP}/_working/_data
        zip -r ocd-scraped-data-${{ inputs.state }}.zip "${{ inputs.state }}"/
        echo "✅ Scraped data zipped successfully"

    - name: Upload OCD Scraped Data as Artifact
      if: inputs.use-scrape-cache != 'true'
      uses: actions/upload-artifact@v4
      with:
        name: ocd-scraped-data-${{ inputs.state }}
        path: ${{ runner.temp }}/_working/_data/ocd-scraped-data-${{ inputs.state }}.zip
        retention-days: 30

    - name: Download OCD Scraped Data for Processing
      uses: actions/download-artifact@v4
      with:
        name: ocd-scraped-data-${{ inputs.state }}
        path: ${{ runner.temp }}/_working/_data/
        if-no-artifact-found: error

    - name: Extract Downloaded Data
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "📦 Extracting downloaded artifact data..."
        cd ${RUNNER_TEMP}/_working/_data
        echo "🔍 Debug: Listing contents of download directory:"
        ls -la
        echo "🔍 Debug: Checking for zip file..."
        if [ -f "ocd-scraped-data-${{ inputs.state }}.zip" ]; then
          echo "📦 Found zip file, extracting..."
          unzip -o "ocd-scraped-data-${{ inputs.state }}.zip"
          echo "✅ Artifact data extracted successfully"
        else
          echo "⚠️ Artifact zip file not found, checking for direct state folder..."
          if [ -d "${{ inputs.state }}" ]; then
            echo "✅ State folder found directly"
          else
            echo "❌ No artifact data found"
            echo "🔍 Debug: Available files and directories:"
            ls -la
            exit 1
          fi
        fi

    - name: Sanitize Data (removes _id and scraped_at)
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🧹 Sanitizing JSON data (removing _id and scraped_at fields)..."
        # Sanitize JSON files with proper error handling
        find ${RUNNER_TEMP}/_working/_data -type f -name "*.json" -exec bash -c '
          if [ -f "{}" ]; then
            jq "del(..|._id?, .scraped_at?)" "{}" > "{}.tmp" && mv "{}.tmp" "{}"
          fi
        ' \;
        echo "✅ Data sanitization completed"

    - name: Install dependencies
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "📦 Installing Python dependencies..."
        pip install pipenv
        pipenv install --deploy
        echo "✅ Dependencies installed successfully"

    - name: Format scraped data
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ./openstates_scraped_data_formatter
      env:
        FORMATTER_INPUT_FOLDER: ${{ runner.temp }}/_working/_data/${{ inputs.state }}
      run: |
        echo "🔧 Formatting scraped data for ${{ inputs.state }}..."
        pipenv run python main.py \
          --state ${{ inputs.state }} \
          --input-folder ${{ runner.temp }}/_working/_data/${{ inputs.state }} \
          --allow-session-fix ${{ inputs.allow-session-fix }}
        echo "✅ Data formatting completed"

    - name: Skip scraping (using latest artifact)
      if: inputs.use-scrape-cache == 'true'
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "⏭️ Skipping scraping - using latest artifact data"
        # Ensure the data directory exists for processing
        mkdir -p ${RUNNER_TEMP}/_working/_data/${{ inputs.state }}
        echo "✅ Using pre-processed data from latest artifact"

    - name: Zip OCD Blockchain Data
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "📦 Zipping OCD blockchain data..."
        zip -r data_output.zip data_output/
        echo "✅ Blockchain data zipped successfully"

    - name: Upload OCD Blockchain Data as Artifact
      uses: actions/upload-artifact@v4
      with:
        name: ocd-blockchain-data-${{ inputs.state }}
        path: data_output.zip
        retention-days: 30