name: 'OpenStates Data Pipeline'
description: 'Complete pipeline to scrape, sanitize, and format OpenStates data'

# USAGE EXAMPLE:
# This composite action requires secrets to be passed as inputs when used by other repositories:
#
# - name: Run OpenStates Pipeline
#   uses: ./
#   with:
#     state: 'tx'
#     github-token: ${{ secrets.GITHUB_TOKEN }}
#     allow-session-fix: 'false'
#
# SECRETS HANDLING:
# - github-token: Pass ${{ secrets.GITHUB_TOKEN }} or a PAT with repo permissions
# - All other secrets must be explicitly passed as inputs

inputs:
  state:
    description: 'State abbreviation (e.g., id, il, tx, ny)'
    required: true
  allow-session-fix:
    description: 'Allow interactive session fixes when session names are missing'
    required: false
    default: 'false'
  use-scrape-cache:
    description: 'Skip scraping and use the most recent artifact from this repository instead'
    required: false
    default: 'false'
  github-token:
    description: 'GitHub token for creating releases and downloading artifacts'
    required: false
    default: '${{ github.token }}'

runs:
  using: 'composite'
  steps:
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.13"

    - name: Setup Working Directory
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🔧 Setting up working directory..."
        # Create working directory outside git repo
        mkdir -p ${RUNNER_TEMP}
        echo "✅ Working directory created at ${RUNNER_TEMP}"

    - name: Check Cache Usage
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        if [ "${{ inputs.use-scrape-cache }}" = "true" ]; then
          echo "🔄 Using scrape cache - skipping scraping steps"
          echo "CACHE_MODE=true" >> $GITHUB_ENV
        else
          echo "🕷️ Proceeding with fresh scraping"
          echo "CACHE_MODE=false" >> $GITHUB_ENV
        fi

    - name: Cache Scrapes
      if: inputs.use-scrape-cache != 'true'
      uses: actions/cache@v4
      with:
        path: ${{ runner.temp }}/cache
        key: cache-scrapes-${{ inputs.state }}-${{ github.sha }}
        restore-keys: |
          cache-scrapes-${{ inputs.state }}-
          cache-scrapes-

    - name: Clone and Build Scrapers from Source
      if: inputs.use-scrape-cache != 'true'
      shell: bash
      working-directory: ${{ github.workspace }}
      run: |
        echo "🔧 Cloning OpenStates scrapers repository..."
        git clone https://github.com/openstates/openstates-scrapers.git
        echo "📦 Building Docker image for scrapers..."
        cd openstates-scrapers
        # Get runner's user and group IDs
        USER_ID=$(id -u)
        GROUP_ID=$(id -g)
        echo "🔧 Building with user ID: $USER_ID and group ID: $GROUP_ID"
        docker build \
          --build-arg USER_ID=$USER_ID \
          --build-arg GROUP_ID=$GROUP_ID \
          -t openstates/scrapers:dev .
        echo "✅ Docker image built successfully"

    - name: Scrape Data
      working-directory: ${{ runner.temp }}
      run: |
        docker pull openstates/scrapers:latest
        mkdir -p /var/tmp/_working
        docker run \
            -v "_working/_data":/opt/openstates/openstates/_data \
            -v "_working/_cache":/opt/openstates/openstates/_cache \
            openstates/scrapers:latest \
            il bills --scrape --fastmode
        tar zcvf scrape-snapshot-nightly.tgz --mode=755 -C _working/_data .

    # - name: Scrape Data
    #   if: inputs.use-scrape-cache != 'true'
    #   shell: bash
    #   working-directory: ${{ runner.temp }}
    #   run: |
    #     echo "🕷️ Starting data scraping for ${{ inputs.state }}..."
    #     # Ensure directories exist
    #     mkdir -p /var/tmp/_working
        
    #     echo "🐳 Running scraper in Docker container..."
    #     # Get current user and group IDs for Docker
    #     echo "🔧 Running Docker with user ID: $USER_ID and group ID: $GROUP_ID"
    #     docker run \
    #         -v "_working/_data":/opt/openstates/openstates/_data \
    #         -v "_working/_cache":/opt/openstates/openstates/_cache \
    #         openstates/scrapers:dev \
    #         ${{ inputs.state }} bills --scrape --fastmode
    #     echo "✅ Data scraping completed"
  
    #     echo "📦 Zipping scraped data for ${{ inputs.state }}..."
    #     tar zcvf scrape-snapshot-nightly.tgz --mode=755 -C _working/_data .
    #     echo "✅ Data zipped successfully"

    # - name: Upload OCD Scraped Data as Artifact
    #   if: inputs.use-scrape-cache != 'true'
    #   uses: actions/upload-artifact@v4
    #   with:
    #     name: ocd-scraped-data-${{ inputs.state }}
    #     path: ${{ runner.temp }}/${{ inputs.state }}.zip
    #     retention-days: 30

    - name: Update Nightly Release
      if: inputs.use-scrape-cache != 'true'
      uses: andelf/nightly-release@v1
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
      with:
        tag_name: nightly
        name: "Nightly release of legislation $$"
        prerelease: false
        files: |
          ${{ runner.temp }}/scrape-snapshot-nightly.tgz

    # - name: Download Release
    #   uses: harrict/release-downloader@v1
    #   with:
    #     token: ${{ inputs.github-token }}
    #     tag: 'nightly'
    #     filename: ${{ inputs.state }}.zip
    #     zipBall: true
    #     extract: true
    #     out-file-path: ${{ runner.temp }}
    
    # - name: Show Output Directory Structure
    #   shell: bash
    #   working-directory: ${{ runner.temp }}
    #   run: |
    #     echo "📂 Output directory structure:"
    #     tree -L 3 -f -n --filelimit 5 ./
    #     echo "✅ Directory listing complete"