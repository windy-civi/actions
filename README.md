# ğŸ›ï¸ Windy Civi: OpenCivicData Blockchain Transformer

[![Python 3.12+](https://img.shields.io/badge/python-3.12+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A GitHub Actions-powered pipeline that scrapes, formats, and extracts text from state and federal legislative data, organizing it into blockchain-style, versioned data structures. But more than that, it's part of a larger mission to make civic data transparent, permanent, and accessible to everyone.

## ğŸ’¡ Why This Matters

Democracy relies on accessible information. Legislative data in the U.S. is often fragmented, inconsistently formatted, and easily lost when administrations change or sites go offline. **Windy Civi** is designed to solve that problem by creating a _decentralized, verifiable record_ of bills, votes, and actionsâ€”structured like a blockchain for traceability and accountability.

This project is inspired by open data movements and maintained by contributors who believe civic infrastructure should be public, durable, and transparent.

## âœ¨ Key Features

- **ğŸ”„ Incremental Processing** - Only processes new or updated bills (no duplicate work!)
- **ğŸ’¾ Auto-Save Failsafe** - Commits progress every 30 minutes (survives GitHub's 6-hour timeout)
- **ğŸ©º Data Quality Monitoring** - Tracks orphaned bills (votes/events without bill data) to catch data issues
- **ğŸ”— Bill-Event Linking** - Automatically connects committee hearings and events to their bills
- **â±ï¸ Timestamp Tracking** - Two-level timestamps for logs and text extraction
- **ğŸ¯ Multi-Format Text Extraction** - XML â†’ HTML â†’ PDF with fallbacks
- **ğŸ”€ Concurrent Job Support** - Multiple actions can run safely with git rebase
- **ğŸ“¦ Modular Actions** - Independent scrape, format, and extract actions

## ğŸ§  On AI and Collaboration

This project was developed with the help of AI tools like **ChatGPT** and **Cursor**, which I used as active collaborators rather than shortcuts. They were especially helpful when exploring architecture decisions, discussing different ways to structure functions, design data flows, and think through tradeoffs that are usually hard to reason about alone.

In many ways, AI became a kind of mentor. It helped me clarify my reasoning, consider alternative approaches, and better understand the 'why' behind every choice. But it also forced me to slow down. I learned to pause, ask questions, and make sure I truly understood what was being built rather than just moving fast.

Most unexpectedly, this process dramatically improved my debugging skills. By breaking problems into smaller conversations and exploring edge cases in real time, I became much more methodical and confident when tracking down issues. In short, coding with AI has been less about automationâ€”and more about becoming a sharper, more reflective engineer.

## ğŸ—ï¸ Repository Structure

```
â”œâ”€â”€ actions/
â”‚   â”œâ”€â”€ scrape/              # Scrape Action (job 1)
â”‚   â”‚   â””â”€â”€ action.yml       # Docker-based scraping
â”‚   â”œâ”€â”€ format/              # Format Action (job 2)
â”‚   â”‚   â””â”€â”€ action.yml       # Data formatting + linking
â”‚   â””â”€â”€ extract/             # Text Extraction Action (independent)
â”‚       â””â”€â”€ action.yml       # Extract text from PDFs/XMLs/HTML
â”œâ”€â”€ scrape_and_format/       # Core formatting logic
â”‚   â”œâ”€â”€ handlers/            # Bill, vote, event processors
â”‚   â”œâ”€â”€ postprocessors/      # Event linking, placeholder cleanup
â”‚   â””â”€â”€ utils/               # Path building, timestamps, I/O
â”œâ”€â”€ text_extraction/         # Text extraction modules
â”‚   â””â”€â”€ utils/               # XML, HTML, PDF parsers
â”œâ”€â”€ docs/                    # Documentation + example workflows
â”‚   â”œâ”€â”€ orphan_tracking.md   # Orphan placeholder guide
â”‚   â””â”€â”€ example-caller-*.yml # Sample GitHub workflows
â”œâ”€â”€ testing/                 # Test suites + sample data
â”œâ”€â”€ Pipfile                  # Python dependencies
â””â”€â”€ README.md
```

## ğŸš€ GitHub Actions Workflow

This pipeline uses **three modular actions** that work together but run independently:

### ğŸ“Š The Scrape & Format Workflow

These two actions run as **separate jobs in the same workflow**, passing data via artifacts:

#### **Action 1: Scrape** â†’ Scrapes legislative data

- Docker-based scraping using OpenStates scrapers
- Creates artifact for next job
- Nightly releases (rolling + immutable archives)

#### **Action 2: Format** â†’ Organizes into blockchain structure

- Downloads scrape artifact
- Incremental processing (only new/updated bills)
- Automatic session detection via API
- Links events to bills and sessions
- **Cleans up placeholder files** and tracks orphans
- Concurrent job support (git pull + rebase)

ğŸ’¡ **Why separate jobs?** Better debugging, preserved scrape data if formatting fails, and clearer logs.

### ğŸ”¤ The Text Extraction Action (Independent)

This runs as a **completely separate workflow**, usually a few hours after scrape+format:

#### **Action 3: Extract Text** â†’ Extracts readable bill text

- Incremental processing (skips already-extracted bills)
- Auto-save every 30 minutes (prevents timeout data loss)
- Multi-format extraction (XML > HTML > PDF)
- Resumable after GitHub's 6-hour timeout
- Independent schedule (avoids timeouts)

ğŸ’¡ **Why separate?** Text extraction can take 4-6 hours for large datasets. Running independently allows it to restart and prevents blocking the scrape/format workflow.

## ğŸ“– For Caller Repositories

Setting up a state pipeline? Everything you need is in the **[docs/for-caller-repos/](docs/for-caller-repos/)** folder:

- **[README_TEMPLATE.md](docs/for-caller-repos/README_TEMPLATE.md)** - Complete setup guide for state pipelines
- **[example-caller-scrape-format.yml](docs/for-caller-repos/example-caller-scrape-format.yml)** - Scrape + format workflow
- **[example-caller-text-extraction.yml](docs/for-caller-repos/example-caller-text-extraction.yml)** - Text extraction workflow

See the [docs/](docs/) folder for additional technical guides.

## ğŸ“Š Output Structure

```
country:us/
â””â”€â”€ state:usa/                           # state:usa for federal, state:il for Illinois, etc.
    â””â”€â”€ sessions/119/
        â”œâ”€â”€ bills/
        â”‚   â””â”€â”€ HR1234/
        â”‚       â”œâ”€â”€ metadata.json           # Bill data + processing timestamps
        â”‚       â”œâ”€â”€ logs/                   # Action logs + vote events
        â”‚       â”‚   â”œâ”€â”€ 20250101T120000Z_introduced.json
        â”‚       â”‚   â””â”€â”€ 20250115T143000Z_vote_event_passed.json
        â”‚       â””â”€â”€ files/                  # Bill text
        â”‚           â”œâ”€â”€ HR1234_text.xml     # Original XML
        â”‚           â”œâ”€â”€ HR1234_text.txt     # Extracted text
        â”‚           â””â”€â”€ HR1234_text.pdf     # PDF fallback
        â””â”€â”€ events/                         # Hearing/committee events
            â””â”€â”€ 20250325T100000Z_hearing.json

.windycivi/                              # Pipeline metadata
â”œâ”€â”€ errors/
â”‚   â”œâ”€â”€ missing_session/
â”‚   â”œâ”€â”€ text_extraction_errors/
â”‚   â”œâ”€â”€ event_archive/
â”‚   â””â”€â”€ orphaned_placeholders_tracking.json  # Data quality monitoring
â”œâ”€â”€ bill_session_mapping.json
â”œâ”€â”€ sessions.json
â””â”€â”€ latest_timestamp_seen.txt            # Last processed timestamp
```

### ğŸ” Processing Metadata Example

Each bill's `metadata.json` includes `_processing` timestamps for incremental updates:

```json
{
  "identifier": "HR 1234",
  "title": "Example Bill",
  "_processing": {
    "logs_latest_update": "2025-01-15T14:30:00Z",
    "text_extraction_latest_update": "2025-01-16T08:00:00Z"
  },
  "actions": [
    {
      "description": "Introduced in House",
      "date": "2025-01-01",
      "_processing": {
        "log_file_created": "2025-01-01T12:00:00Z"
      }
    }
  ]
}
```

## ğŸ©º Data Quality Monitoring

The pipeline automatically tracks **orphaned bills** - bills that have vote events or hearings but no actual bill data. This helps identify data quality issues like:

- Typos in bill identifiers (e.g., "HR 999" vs "HR999")
- Bills that weren't scraped but had related activity
- Timing issues (bill not scraped yet)

Check `.windycivi/errors/orphaned_placeholders_tracking.json` to see:

- Which bills are orphaned
- How long they've been orphaned (`first_seen`, `last_seen`)
- How many times they've appeared (`occurrence_count`)
- What data exists for them (vote counts, event counts)

**Chronic orphans** (seen 3+ times) are flagged for investigation. When a bill finally arrives, it's automatically resolved! ğŸ‰

ğŸ“– See [docs/orphan_tracking.md](docs/orphan_tracking.md) for more details.

## âš™ï¸ Local Setup

**Requirements:**

- Python 3.12+
- pipenv
- Docker (for scraping)

**Installation:**

```bash
# Clone the repository
cd toolkit

# Install dependencies
pipenv install

# Run formatting (requires pre-scraped data)
pipenv run python scrape_and_format/main.py \
  --state usa \
  --openstates-data-folder /path/to/scraped/data \
  --git-repo-folder /path/to/output
```

**For scraping**, use the Docker-based action or OpenStates scrapers directly.

## ğŸ§ª Testing

Run the test suite to verify everything works:

```bash
# Test incremental processing
bash testing/incremental/run_test.sh

# Test orphan placeholder cleanup
python testing/scripts/test_placeholder_cleanup.py
```

## ğŸ“š Documentation

- **[Orphan Tracking Guide](docs/orphan_tracking.md)** - Understanding orphaned bills
- **[Incremental Processing](docs/incremental_processing/)** - How incremental updates work
- **[Example Workflows](docs/)** - Ready-to-use GitHub Actions examples

## ğŸŒ Project Vision

Windy Civi's long-term goal is to create a **permanent, blockchain-style public archive** for all legislative dataâ€”linking together OpenStates scrapers, GitHub repositories, and decentralized storage. Think of it as a civic data backbone for researchers, journalists, and citizens.

## ğŸ¤ Contributing

This project is part of the broader **Windy Civi** ecosystemâ€”a civic tech initiative based in Chicago focused on open data and transparency. We welcome collaborators, contributors, and curious minds.

- ğŸ™ [Open an Issue](https://github.com/windy-civi/toolkit/issues)
- ğŸ“¬ Submit a PR
- ğŸŒ± Help design future civic pipelines
- ğŸ’¬ Join the conversation about civic tech

## ğŸ“œ License

MIT License - feel free to use, modify, and build upon this work.

**Built with care, code, and curiosity.** ğŸ›ï¸âœ¨

---

_Part of the [Windy Civi](https://github.com/windy-civi) ecosystem - Making civic data transparent and accessible._
